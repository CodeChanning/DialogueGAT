{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Path\n",
    "\n",
    "The following code sets data path, replace them with your own dir path. You can find samples of data in the corresponding path.\n",
    "\n",
    "* Put the original earning conference call transcript htmls from seeking alpha in `origin_path`. \n",
    "* Download the stock price file from WRDS and put it in the current folder (see `price_sample.csv`).  \n",
    "* Download `glove.840B.300d.zip` from <https://nlp.stanford.edu/projects/glove/> and unzip it in the current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "origin_path = \"origin_sample\"\n",
    "\n",
    "html_path = \"html_sample\"\n",
    "json_path = \"json_sample\"\n",
    "price_file = \"price_sample.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs\n",
    "\n",
    "if you don't have the libraries listed below, install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "from dateutil.parser import parse\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_time(s):\n",
    "    dt = parse(s, fuzzy=True, ignoretz=True)\n",
    "    fdt = \"{}{:02d}{:02d}\".format(dt.year, dt.month, dt.day)\n",
    "    return fdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Rename Download HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcca339e3e634e2f9faec8e9cebfeb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = os.listdir(origin_path)\n",
    "\n",
    "k = 0\n",
    "\n",
    "with tqdm(files) as tq:\n",
    "    for file in tq:\n",
    "        if not os.path.isdir(file):\n",
    "            # if \"transcript\" not in file:\n",
    "            #     continue\n",
    "            with open(origin_path + \"/\" + file) as f:\n",
    "                soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "                ta = soup.select(\"div.a-info.clearfix time\")\n",
    "    #             na1 = soup.select(\"span#about_primary_stocks a\")\n",
    "                na2 = soup.select(\"#a-body > p:nth-child(-n+3)\")\n",
    "\n",
    "                if not ta or not na2:\n",
    "                    continue\n",
    "                    \n",
    "                t = parse_time(ta[0].string)\n",
    "    #             n1 = na1[0][\"href\"].split(\"/\")[-1]\n",
    "    \n",
    "                flag = False\n",
    "                for n in na2:\n",
    "                    nt = n.text.strip()\n",
    "                    \n",
    "                    if not nt:\n",
    "                        continue\n",
    "\n",
    "                    ns = re.findall(r'[(](.*?)[)]', nt)\n",
    "                    if not ns:\n",
    "                        continue\n",
    "                        \n",
    "                    n2 = ns[0].split(\":\")[-1].strip()\n",
    "                    flag = True\n",
    "                    break\n",
    "                \n",
    "                if not flag or not n2:\n",
    "                    continue\n",
    "\n",
    "                key = t + \"_\" + n2\n",
    "                \n",
    "            \n",
    "            shutil.copyfile(origin_path + \"/\" + file, html_path + \"/\" + key + \".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f2977cda7449e680ad6d48ab49709a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = os.listdir(html_path)\n",
    "not_found_rdq = []\n",
    "    \n",
    "for file in tqdm(files):\n",
    "    if not os.path.isdir(file):\n",
    "#         print(file)\n",
    "        rdq_key = file.split(\".\")[0]\n",
    "        date, ticker = rdq_key.split(\"_\")\n",
    "        with open(html_path + \"/\" + file) as f:\n",
    "            ret = {\n",
    "                \"date\": date,\n",
    "                \"ticker\": ticker,\n",
    "                \"participant\": [],\n",
    "                \"transcript\": []\n",
    "            }\n",
    "            key, values, tp = None, [], \"pre\"\n",
    "            soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "\n",
    "            ps = soup.select(\"div#a-body p\")\n",
    "            for p in ps:\n",
    "                if p.get_text().strip() == \"Question-and-Answer Session\":\n",
    "                    ret[\"transcript\"].append({\"name\": key, \"speech\": values, \"type\": tp})\n",
    "                    tp = \"qa\"\n",
    "                    key = None\n",
    "                    continue\n",
    "\n",
    "                if p.find(\"strong\"):\n",
    "                    if not key or not values:\n",
    "                        values = []\n",
    "                        key = p.get_text().strip()\n",
    "                        continue\n",
    "\n",
    "                    if key == \"Executives\" or key == \"Analysts\":\n",
    "                        for v in values:\n",
    "    #                             print(v)\n",
    "                            items = v.replace(\"â€“\", \"-\").split(\" - \")\n",
    "                            n, d = items[0], \"-\".join(items[1: ])\n",
    "                            ret[\"participant\"].append({\"name\": n, \"description\": d, \"position\": key})\n",
    "                    elif key == \"Question-and-Answer Session\":\n",
    "                        tp = \"qa\"\n",
    "                    else:\n",
    "                        ret[\"transcript\"].append({\"name\": key, \"speech\": values, \"type\": tp})\n",
    "                    values = []\n",
    "                    key = p.get_text().strip()\n",
    "                else:\n",
    "                    v = p.get_text()\n",
    "                    v = v.replace(\"[\", \"\").replace(\"]\", \"\").strip()\n",
    "                    if v:\n",
    "                        values.append(v)\n",
    "\n",
    "            if key and values:\n",
    "                ret[\"transcript\"].append({\"name\": key, \"speech\": values, \"type\": tp})\n",
    "                \n",
    "        fname = date + \"_\" + ticker + \".json\"\n",
    "        with open(json_path + \"/\" + fname, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(ret, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(not_found_rdq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_glove(file):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(file, \"r\")\n",
    "    glove = {}\n",
    "    for line in tqdm(f):\n",
    "        split_lines = line.split(\" \")\n",
    "        word = split_lines[0]\n",
    "        word_embedding = np.array([float(value) for value in split_lines[1:]])\n",
    "        glove[word] = word_embedding\n",
    "\n",
    "    print(len(glove), \" words loaded!\")\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(\"\\W+\", \" \", s).replace(\"_\", \" \")\n",
    "    s = re.sub(\"\\s+\", \" \", s)\n",
    "    s = s.split(\" \")\n",
    "    words = stopwords.words(\"english\")\n",
    "    s = [w for w in s if w not in words]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(data, vectors):\n",
    "    vocab, max_sen_len, max_p_len = {}, 0, 0\n",
    "    for key in data:\n",
    "        for t in data[key][\"transcript\"]:\n",
    "            sen_len = [0]\n",
    "            for s in t[\"speech\"]:\n",
    "                length = 0\n",
    "                for w in s:\n",
    "                    if w not in vectors:\n",
    "                        continue\n",
    "                    if w not in vocab:\n",
    "                        vocab[w] = 0\n",
    "                    vocab[w] += 1\n",
    "                    length += 1\n",
    "                sen_len.append(length)\n",
    "            max_sen_len = max(max_sen_len, max(sen_len))\n",
    "            max_p_len = max(max_p_len, sum(sen_len))\n",
    "    idx2words = list(vocab.keys())\n",
    "    word2idx = {w: i + 1 for i, w in enumerate(idx2words)}\n",
    "    W = [np.zeros(300)] + [vectors[w] for w in idx2words]\n",
    "    word2idx = {w: i for i, w in enumerate(idx2words)}\n",
    "    W = [vectors[w] for w in idx2words]\n",
    "    return vocab, word2idx, W, max_sen_len, max_p_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    files = os.listdir(path)\n",
    "    data = {}\n",
    "\n",
    "    for file in files:\n",
    "        if not os.path.isdir(file):\n",
    "            key = file.replace(\".json\", \"\")\n",
    "            if key == \"20180504_SM\" or key == \"20150729_BEN\":\n",
    "                continue\n",
    "            with open(path + \"/\" + file, \"r\") as f:\n",
    "                d = json.load(f)\n",
    "            data[key] = d\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_label(price_file, data):\n",
    "    df = pd.read_csv(price_file)\n",
    "    df = df.fillna(0)\n",
    "    df[\"date\"] = df[\"date\"].apply(str)\n",
    "\n",
    "    tic2id = {}\n",
    "    for index, row in tqdm(df[[\"TICKER\", \"PERMNO\"]].drop_duplicates().iterrows()):\n",
    "        tic2id[row[\"TICKER\"]] = row[\"PERMNO\"]\n",
    "\n",
    "    for key in tqdm(data):\n",
    "        date, tic = key.split(\"_\")\n",
    "        ndate = \"{}/{}/{}\".format(date[4:6], date[6:], date[:4])\n",
    "        if tic == \"WELL\":\n",
    "            tic = \"HCN\"\n",
    "        # price = []\n",
    "        if tic not in tic2id:\n",
    "            print(tic)\n",
    "            continue\n",
    "\n",
    "        idx = df[\n",
    "            ((df[\"PERMNO\"] == tic2id[tic]) | (df[\"TICKER\"] == tic))\n",
    "            & (df[\"date\"] == ndate)\n",
    "        ].index[0]\n",
    "\n",
    "        pprice = list(reversed(df.loc[idx - 31 : idx - 1, \"RETX\"].tolist()))\n",
    "        price = df.loc[idx : idx + 30, \"RETX\"].tolist()\n",
    "\n",
    "        if not price:\n",
    "            print(key)\n",
    "\n",
    "        pprice = [float(p) for p in pprice]\n",
    "        price = [float(p) for p in price]\n",
    "\n",
    "        data[key][\"label\"] = {\n",
    "            -3: np.log(np.std(pprice[:4], ddof=1)),\n",
    "            -7: np.log(np.std(pprice[:8], ddof=1)),\n",
    "            -15: np.log(np.std(pprice[:16], ddof=1)),\n",
    "            -30: np.log(np.std(pprice, ddof=1)),\n",
    "            3: np.log(np.std(price[:4], ddof=1)),\n",
    "            7: np.log(np.std(price[:8], ddof=1)),\n",
    "            15: np.log(np.std(price[:16], ddof=1)),\n",
    "            30: np.log(np.std(price, ddof=1)),\n",
    "        }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(data, glove):\n",
    "    for key in data:\n",
    "        for j, t in enumerate(data[key][\"transcript\"]):\n",
    "            new_speech = []\n",
    "            for k, s in enumerate(t[\"speech\"]):\n",
    "                ns = clean_text(data[key][\"transcript\"][j][\"speech\"][k])\n",
    "                if ns:\n",
    "                    new_speech.append(ns)\n",
    "            data[key][\"transcript\"][j][\"speech\"] = new_speech\n",
    "\n",
    "    vocab, word2idx, W, max_sen_len, max_p_len = get_embedding(data, glove)\n",
    "    # for key in data:\n",
    "    #     with open(\"/home/sangyx/data/ecc/json_label/{}.json\".format(key), \"w\") as f:\n",
    "    #         json.dump(data[key], f)\n",
    "    return vocab, word2idx, W, max_sen_len, max_p_len, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e1221416834971865ab41f167ad26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196016  words loaded!\n"
     ]
    }
   ],
   "source": [
    "glove = load_glove(\"glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39231f00ad44eef8284158c86d6d6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb7ac274c93407bb689f7c664f2ba84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = read_files(json_path)\n",
    "vocab, word2idx, W, max_sen_len, max_p_len, data = preprocess(data, glove)\n",
    "data = get_label(\"price_sample.csv\", data)\n",
    "with open(\"data_sample.pkl\", \"wb\") as f:\n",
    "    pickle.dump([data, vocab, word2idx, W, max_p_len, max_sen_len], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
