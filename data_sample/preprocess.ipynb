{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Path\n",
    "\n",
    "The following code sets data path, replace them with your own dir path. You can find samples of data in the corresponding path.\n",
    "\n",
    "* Put the original earning conference call transcript htmls from seeking alpha in `origin_path`. \n",
    "* Download the stock price file from WRDS and put it in the current folder (see `price_sample.csv`).  \n",
    "* Download `glove.840B.300d.zip` from <https://nlp.stanford.edu/projects/glove/> and unzip it in the current folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load in new samples\n",
    "\n",
    "- Download earnings conference call transcript html from seeking alpha (seekingalpha.com, search for desired company) and put the .html in origin folder\n",
    "- Download stock price file from WRDS (https://library.bu.edu/c.php?g=541045&p=3705854) for the same company and time period and put in this folder\n",
    "- Download glove model\n",
    "- Run all of these notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "origin_path = \"origin_sample\"\n",
    "\n",
    "html_path = \"html_sample\"\n",
    "json_path = \"json_sample\"\n",
    "price_file = \"price_sample.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs\n",
    "\n",
    "if you don't have the libraries listed below, install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "from dateutil.parser import parse\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_time(s):\n",
    "    dt = parse(s, fuzzy=True, ignoretz=True)\n",
    "    fdt = \"{}{:02d}{:02d}\".format(dt.year, dt.month, dt.day)\n",
    "    return fdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://seeking-alpha.p.rapidapi.com/transcripts/v2/get-details\"\n",
    "\n",
    "ids = [3131236, 3385105, 3626926, 3845726, 2766875, 3031196, 3283865, 3531246, 3596936, 3846086, 3353155, 3098186]\n",
    "\n",
    "id = 3098186\n",
    "querystring = {\"id\":str(id)}\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Key\": \"\",\n",
    "\t\"X-RapidAPI-Host\": \"seeking-alpha.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "publishDate = response.json()['data']['attributes']['publishOn']\n",
    "date = publishDate[:10] + \" \" + publishDate[11:19] + \" \" + publishDate[21:]\n",
    "content = response.json()['data']['attributes']['content']\n",
    "url = response.json()['data']['links']['canonical']\n",
    "template = f'<article><header><meta content=\"{url}\" itemType=\"https://schema.org/WebPage\" itemid=\"{url}\" itemprop=\"mainEntityOfPage\" itemscope=\"\" /></header>\\n<div class=\"sa-art article-width\" id=\"a-body\" itemprop=\"articleBody\"><div class=\"a-info clearfix\"><time content=\"{publishDate}\">{publishDate}</time></div><p class=\"p p1\">{content[3:content.find(\"</p>\")]}</p>{content[content.find(\"</p>\")+5:]}</article>'\n",
    "pattern = r'(</[^>]+>)\\s'\n",
    "split_html = re.sub(pattern, r'\\1\\n', template)\n",
    "with open(f'origin_sample/{response.json()[\"data\"][\"id\"]}.html', 'w') as file:\n",
    "\tfile.write(split_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishDate = response.json()['data']['attributes']['publishOn']\n",
    "date = publishDate[:10] + \" \" + publishDate[11:19] + \" \" + publishDate[21:]\n",
    "content = response.json()['data']['attributes']['content']\n",
    "url = response.json()['data']['links']['canonical']\n",
    "template = f'<article><header><meta content=\"{url}\" itemType=\"https://schema.org/WebPage\" itemid=\"{url}\" itemprop=\"mainEntityOfPage\" itemscope=\"\" /></header>\\n<div class=\"sa-art article-width\" id=\"a-body\" itemprop=\"articleBody\"><div class=\"a-info clearfix\"><time content=\"{publishDate}\">{publishDate}</time></div><p class=\"p p1\">{content[3:content.find(\"</p>\")]}</p>{content[content.find(\"</p>\")+5:]}</article>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(</[^>]+>)\\s'\n",
    "split_html = re.sub(pattern, r'\\1\\n', template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'origin_sample/{response.json()[\"data\"][\"id\"]}.html', 'w') as file:\n",
    "    file.write(split_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Rename Download HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 20.81it/s]\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(origin_path)\n",
    "\n",
    "k = 0\n",
    "\n",
    "with tqdm(files) as tq:\n",
    "    for file in tq:\n",
    "        if not os.path.isdir(file):\n",
    "            # if \"transcript\" not in file:\n",
    "            #     continue\n",
    "            with open(origin_path + \"/\" + file) as f:\n",
    "                soup = BeautifulSoup(f.read())#, \"lxml\")\n",
    "                ta = soup.select(\"div.a-info.clearfix time\")\n",
    "    #             na1 = soup.select(\"span#about_primary_stocks a\")\n",
    "                na2 = soup.select(\"#a-body > p:nth-child(-n+3)\")\n",
    "\n",
    "                if not ta or not na2:\n",
    "                    continue\n",
    "                    \n",
    "                t = parse_time(ta[0].string)\n",
    "    #             n1 = na1[0][\"href\"].split(\"/\")[-1]\n",
    "    \n",
    "                flag = False\n",
    "                for n in na2:\n",
    "                    nt = n.text.strip()\n",
    "                    \n",
    "                    if not nt:\n",
    "                        continue\n",
    "\n",
    "                    ns = re.findall(r'[(](.*?)[)]', nt)\n",
    "                    if not ns:\n",
    "                        continue\n",
    "                        \n",
    "                    n2 = ns[0].split(\":\")[-1].strip()\n",
    "                    flag = True\n",
    "                    break\n",
    "                \n",
    "                if not flag or not n2:\n",
    "                    continue\n",
    "\n",
    "                key = t + \"_\" + n2\n",
    "                \n",
    "            \n",
    "            shutil.copyfile(origin_path + \"/\" + file, html_path + \"/\" + key + \".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 22.10it/s]\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(html_path)\n",
    "not_found_rdq = []\n",
    "    \n",
    "for file in tqdm(files):\n",
    "    if not os.path.isdir(file):\n",
    "#         print(file)\n",
    "        rdq_key = file.split(\".\")[0]\n",
    "        date, ticker = rdq_key.split(\"_\")\n",
    "        with open(html_path + \"/\" + file) as f:\n",
    "            ret = {\n",
    "                \"date\": date,\n",
    "                \"ticker\": ticker,\n",
    "                \"participant\": [],\n",
    "                \"transcript\": []\n",
    "            }\n",
    "            key, values, tp = None, [], \"pre\"\n",
    "            soup = BeautifulSoup(f.read())#, \"lxml\")\n",
    "\n",
    "            ps = soup.select(\"div#a-body p\")\n",
    "            for p in ps:\n",
    "                if p.get_text().strip() == \"Question-and-Answer Session\":\n",
    "                    ret[\"transcript\"].append({\"name\": key, \"speech\": values, \"type\": tp})\n",
    "                    tp = \"qa\"\n",
    "                    key = None\n",
    "                    continue\n",
    "\n",
    "                if p.find(\"strong\"):\n",
    "                    if not key or not values:\n",
    "                        values = []\n",
    "                        key = p.get_text().strip()\n",
    "                        continue\n",
    "\n",
    "                    if key == \"Executives\" or key == \"Analysts\":\n",
    "                        for v in values:\n",
    "    #                             print(v)\n",
    "                            items = v.replace(\"–\", \"-\").split(\" - \")\n",
    "                            n, d = items[0], \"-\".join(items[1: ])\n",
    "                            ret[\"participant\"].append({\"name\": n, \"description\": d, \"position\": key})\n",
    "                    elif key == \"Question-and-Answer Session\":\n",
    "                        tp = \"qa\"\n",
    "                    else:\n",
    "                        ret[\"transcript\"].append({\"name\": key, \"speech\": values, \"type\": tp})\n",
    "                    values = []\n",
    "                    key = p.get_text().strip()\n",
    "                else:\n",
    "                    v = p.get_text()\n",
    "                    v = v.replace(\"[\", \"\").replace(\"]\", \"\").strip()\n",
    "                    if v:\n",
    "                        values.append(v)\n",
    "\n",
    "            if key and values:\n",
    "                ret[\"transcript\"].append({\"name\": key, \"speech\": values, \"type\": tp})\n",
    "                \n",
    "        fname = date + \"_\" + ticker + \".json\"\n",
    "        with open(json_path + \"/\" + fname, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(ret, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(not_found_rdq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_glove(file):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(file, \"r\", encoding=\"utf8\")\n",
    "    glove = {}\n",
    "    for line in tqdm(f):\n",
    "        split_lines = line.split(\" \")\n",
    "        word = split_lines[0]\n",
    "        word_embedding = np.array([float(value) for value in split_lines[1:]])\n",
    "        glove[word] = word_embedding\n",
    "\n",
    "    print(len(glove), \" words loaded!\")\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(\"\\W+\", \" \", s).replace(\"_\", \" \")\n",
    "    s = re.sub(\"\\s+\", \" \", s)\n",
    "    s = s.split(\" \")\n",
    "    words = stopwords.words(\"english\")\n",
    "    s = [w for w in s if w not in words]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(data, vectors):\n",
    "    vocab, max_sen_len, max_p_len = {}, 0, 0\n",
    "    for key in data:\n",
    "        for t in data[key][\"transcript\"]:\n",
    "            sen_len = [0]\n",
    "            for s in t[\"speech\"]:\n",
    "                length = 0\n",
    "                for w in s:\n",
    "                    if w not in vectors:\n",
    "                        continue\n",
    "                    if w not in vocab:\n",
    "                        vocab[w] = 0\n",
    "                    vocab[w] += 1\n",
    "                    length += 1\n",
    "                sen_len.append(length)\n",
    "            max_sen_len = max(max_sen_len, max(sen_len))\n",
    "            max_p_len = max(max_p_len, sum(sen_len))\n",
    "    idx2words = list(vocab.keys())\n",
    "    word2idx = {w: i + 1 for i, w in enumerate(idx2words)}\n",
    "    W = [np.zeros(300)] + [vectors[w] for w in idx2words]\n",
    "    word2idx = {w: i for i, w in enumerate(idx2words)}\n",
    "    W = [vectors[w] for w in idx2words]\n",
    "    return vocab, word2idx, W, max_sen_len, max_p_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    files = os.listdir(path)\n",
    "    data = {}\n",
    "\n",
    "    for file in files:\n",
    "        if not os.path.isdir(file):\n",
    "            key = file.replace(\".json\", \"\")\n",
    "            if key == \"20180504_SM\" or key == \"20150729_BEN\":\n",
    "                continue\n",
    "            with open(path + \"/\" + file, \"r\") as f:\n",
    "                d = json.load(f)\n",
    "            data[key] = d\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_label(price_file, data):\n",
    "    df = pd.read_csv(price_file)\n",
    "    df = df.fillna(0)\n",
    "    df[\"date\"] = df[\"date\"].apply(str)\n",
    "\n",
    "    tic2id = {}\n",
    "    for index, row in tqdm(df[[\"TICKER\", \"PERMNO\"]].drop_duplicates().iterrows()):\n",
    "        tic2id[row[\"TICKER\"]] = row[\"PERMNO\"]\n",
    "\n",
    "    for key in tqdm(data):\n",
    "        date, tic = key.split(\"_\")\n",
    "        ndate = \"{}/{}/{}\".format(date[4:6], date[6:], date[:4])\n",
    "        if tic == \"WELL\":\n",
    "            tic = \"HCN\"\n",
    "        # price = []\n",
    "        if tic not in tic2id:\n",
    "            print(tic)\n",
    "            continue\n",
    "\n",
    "        idx = df[\n",
    "            ((df[\"PERMNO\"] == tic2id[tic]) | (df[\"TICKER\"] == tic))\n",
    "            & (df[\"date\"] == ndate)\n",
    "        ].index[0]\n",
    "\n",
    "        pprice = list(reversed(df.loc[idx - 31 : idx - 1, \"RETX\"].tolist()))\n",
    "        price = df.loc[idx : idx + 30, \"RETX\"].tolist()\n",
    "\n",
    "        if not price:\n",
    "            print(key)\n",
    "\n",
    "        pprice = [float(p) for p in pprice]\n",
    "        price = [float(p) for p in price]\n",
    "\n",
    "        data[key][\"label\"] = {\n",
    "            -3: np.log(np.std(pprice[:4], ddof=1)),\n",
    "            -7: np.log(np.std(pprice[:8], ddof=1)),\n",
    "            -15: np.log(np.std(pprice[:16], ddof=1)),\n",
    "            -30: np.log(np.std(pprice, ddof=1)),\n",
    "            3: np.log(np.std(price[:4], ddof=1)),\n",
    "            7: np.log(np.std(price[:8], ddof=1)),\n",
    "            15: np.log(np.std(price[:16], ddof=1)),\n",
    "            30: np.log(np.std(price, ddof=1)),\n",
    "        }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(data, glove):\n",
    "    for key in data:\n",
    "        for j, t in enumerate(data[key][\"transcript\"]):\n",
    "            new_speech = []\n",
    "            for k, s in enumerate(t[\"speech\"]):\n",
    "                ns = clean_text(data[key][\"transcript\"][j][\"speech\"][k])\n",
    "                if ns:\n",
    "                    new_speech.append(ns)\n",
    "            data[key][\"transcript\"][j][\"speech\"] = new_speech\n",
    "\n",
    "    vocab, word2idx, W, max_sen_len, max_p_len = get_embedding(data, glove)\n",
    "    # for key in data:\n",
    "    #     with open(\"/home/sangyx/data/ecc/json_label/{}.json\".format(key), \"w\") as f:\n",
    "    #         json.dump(data[key], f)\n",
    "    return vocab, word2idx, W, max_sen_len, max_p_len, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "523396it [00:43, 12010.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523395  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove = load_glove(\"glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mikad\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 911.41it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 2222.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACN\n",
      "ACN\n",
      "AMZN\n",
      "BLL\n",
      "ACN\n",
      "AMZN\n",
      "BLL\n",
      "ACN\n",
      "AMZN\n",
      "BLL\n",
      "AMZN\n",
      "MGCD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = read_files(json_path)\n",
    "vocab, word2idx, W, max_sen_len, max_p_len, data = preprocess(data, glove)\n",
    "data = get_label(\"price_sample.csv\", data)\n",
    "with open(\"data_sample.pkl\", \"wb\") as f:\n",
    "    pickle.dump([data, vocab, word2idx, W, max_p_len, max_sen_len], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
